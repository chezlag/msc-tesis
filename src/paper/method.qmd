# Method {#sec-method}

I analyze the impact of receiving electronic invoices on several firm outcomes following a difference-in-differences strategy. For my main analysis, I use a two-way fixed effects (TWFE) specification:

$$
y_{it} = \alpha_{i} + \tau_{t} + \beta_{post}\ D_{it} + u_{it}
$${#eq-twfe}

where $y_{it}$ is firm $i$'s outcome in year $t$, $\alpha_{i}$ and $\tau_{t}$ are firm and year fixed effects, and $D_{it}$ is an indicator variable valued 1 when firm $i$ has received any e-invoices by year $t$. Total policy impact is captured by $\beta_{post}$, which represents the average effect of receiving e-invoices experienced by all companies that receive them at some point.

I compare the variation in outcomes of firms that receive e-invoices (treated) with the variation in outcomes of firms that have not yet received e-invoices (controls). The main identifying assumption is that treated firms and the control group would have evolved in parallel in absence of e-invoicing.

To provide evidence to support the assumption of parallel trends and to analyze the dynamic effect of receiving an e-invoice, I estimate an event study specification of @eq-twfe:

$$
y_{it} = \alpha_{i} + \tau_{t}+ \sum_{l\neq -1, l = -6}^{l = 3} \beta_{l}\cdot \mathbb{1}(R_{it} = l) + u_{it}
$${#eq-es}

where $R_{it}$ indicates the years since firm $i$ received its first e-invoice. 
The difference in outcomes between treated and control firms $l$ periods after treatment is captured by the $\beta_{l}$ coefficients. Event $l=-1$ is excluded, thereby normalizing the rest of the coefficients $\beta_{l \neq -1}$ to the event right before treatment begins. Given firms first start receiving e-invoices in 2012, the data has at most six pre-treatment periods and three post-treatment periods.

My main outcome variables are input VAT, output VAT and net VAT liability (= max{Output VAT - Input VAT, 0}). I follow @chenLogsZerosProblems2023's recommendations for dealing with dependent variables in logarithms when there is a non-trivial percentage of zeros in their distribution. [^chen-roth] One of their proposed alternatives –the one I follow– involves rescaling the outcome variable $Y_{it}$ with respect to its minimum value $Y_{min}$ and assigning a specific value $\varepsilon$ to changes in the extensive margin. Formally, I define:

[^chen-roth]: The functional form of the dependent variables is no trivial matter. @chenLogsZerosProblems2023 point out the pitfalls of using quasi-logarithmic transformations defined at zero-such as $\log(Y+1)$ or $\text{arcsinh}(Y)$-of non-negative dependent variables. Interpreting treatment effects estimated using these types of transformations as percentage effects is a mistake, as they depend $Y$'s unit of measurement. If the treatment has an effect on the extensive margin, the authors show that it is possible to obtain treatment effects of any magnitude by rescaling $Y$ before applying the quasi-logarithmic transformation.


$$
y_{it} =
\begin{cases}
  \log(\frac{Y_{it}}{Y_{min}}) & \text{si } Y_{it}>0 \\
  -\varepsilon                 & \text{si } Y_{it} = 0
\end{cases}
$$

In my preferred specification I assume that changes in the extensive margin (from zero to $Y_{min}$) are equivalent to a 10% change in the intensive margin –i.e. $\varepsilon=0.10$–. As a robustness exercise, I test different values for $\varepsilon$ and find that results hold regardless of how much weight I give to changes in the extensive margin. I further transform the dependent variables by top-censoring (*winsorizing*) the outcomes at the 99th percentile, for each year. [^winsorizing]

[^winsorizing]: Upper censoring of outcome variables eliminates the influence of outliers and data entry errors. Even in the absence of errors, censoring the data can be optimal when estimating means in finite samples of asymmetric distributions [@rivestStatisticalPropertiesWinsorized1994]. As a robustness exercise, I run another set of regressions while censoring at the 95th percentile.

I test the hypotheses put forward in [Section @sec-hypotheses] with the @eq-twfe coefficients. That is: I focus on the aggregate effect of the policy to test whether the receipt of e-invoices altered firms' tax compliance.

In addition, I test for heterogeneous effects in three different ways. First, I study whether firms respond divergently with regards to their size by splitting the sample according to firms' asset level before 2011. Second, I analyse whether industries exhibit different responses according to their relative exposure to final consumption, by segmenting the sample according to industries' share of output exchanged with households. Finally, I examine whether industries exhibit different responses according to the relative importance of foreign inputs, by splitting the sample according to the weight of imported inputs in each industry's intermediate consumption.[^industry-summary] Given the limited sample size, in all three cases I split the sample into two groups, depending on whether firms are above or below the median of the variable of interest. I estimate the following specification:

$$
y_{it} = \alpha_{i} + \tau_{t} + \beta_{low}\cdot D_{it} \cdot \text{BelowMedian}_{i} + \beta_{high}\cdot D_{it} \cdot \text{AboveMedian}_{i} + u_{it}
$${#eq-twfe-het}

where $\text{BelowMedian}_{i}$ and $\text{AboveMedian}_{i}$ indicate whether the firm is above or below the median of each of the above variables. I compare $\beta_{low}$ and $\beta_{high}$ to test for heterogeneous effects across the mentioned groups.

[industry-summary]: I classify industries according to the share of their output destined for final household consumption using the Uruguayan Central Bank's 2016 input-output tables. @tbl-industry-summary presents summary statistics for each industry.

Recent advances in the difference-in-differences literature suggest that the traditional estimation strategy with two-way fixed effects may be biased if treatment effects are heterogeneous over time and across treatment cohorts [@callawayDifferenceinDifferencesMultipleTime2021; @dechaisemartinTwoWayFixedEffects2020]. Tests have been developed to measure the severity of the problem [@goodman-baconDifferenceindifferencesVariationTreatment2021] and alternative estimation methods have arisen which circumvent the problem [@dechaisemartinTwoWayFixedEffects2020; @sunEstimatingDynamicTreatment2021; @callawayDifferenceinDifferencesMultipleTime2021; @borusyakRevisitingEventStudy2022]. As a robustness exercise, I replicate the main estimates using the methodology proposed by @callawayDifferenceinDifferencesMultipleTime2021 . This method has the advantage of allowing for arbitrary heterogeneity in treatment effect, regarding both time since treatment begins and across treatment cohorts. Results are presented in @sec-appendix-cs21 in the \href{https://www.dropbox.com/s/8dp2z62gf3w5jkf/Online-appendix.pdf?st=lp7mjeeh&dl=0}{Online Appendix} and discussed in [Section @sec-results-effect-robust].

I cluster standard errors at the firm level, given this is the level at which the treatment is defined. I perform my main estimations in `R` using the `fixest` package [@bergeEfficientEstimationMaximum2018]. I reestimate using @callawayDifferenceinDifferencesMultipleTime2021's methodology using the `did` package [@callawayDidDifferenceDifferences2021], with code implemented by the original authors. I organise the process of data cleaning, estimation and preparation of tables and figures using `Snakemake` [@molderSustainableDataAnalysis2021]. The code for this paper is available at [Github](https://github.com/chezlag/msc-tesis).
